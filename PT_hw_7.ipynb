{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PT_hw_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Урок 7. Рекурентные сети для обработки последовательностей"
      ],
      "metadata": {
        "id": "LO9XEqIPVQNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Домашнее задание"
      ],
      "metadata": {
        "id": "gCm3MDF6VlPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Попробуйте обучить нейронную сеть GRU/LSTM для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
        "\n",
        "2. Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?\n",
        "\n",
        "У кого нет возможности работать через каггл (нет верификации), то можете данные взять по ссылке: https://disk.yandex.ru/d/LV1cYS1orMyRWA"
      ],
      "metadata": {
        "id": "bdf78yokVohc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подключение библиотек"
      ],
      "metadata": {
        "id": "KgMD_AldVxuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sagT1UAWUAm",
        "outputId": "6ed5ed0d-014d-4331-e7f8-4e2ec7df641c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.0-py3-none-any.whl (418 kB)\n",
            "\u001b[K     |████████████████████████████████| 418 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGYSUwHUWZeY",
        "outputId": "c57a1740-55b4-40a6-b14c-e09303d99525"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 20.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchmetrics\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7pFwneHJV8Fr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcP7bRkqYTOJ",
        "outputId": "c5b8cc0b-e29b-4c3d-dcb2-8def3b7022ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLkMa1rVYr7V",
        "outputId": "398962d6-dd53-4ad0-c4ba-93be0f53b403"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOCgFzjEY-Zv",
        "outputId": "9bf0ab36-86f3-4520-92bb-31f93bc9cefa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка и подготовка данных"
      ],
      "metadata": {
        "id": "4oWbwl38XWrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test.csv')"
      ],
      "metadata": {
        "id": "5Q3zvjtzXWA1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_valid = train_test_split(train_df, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "RH7B5Ku3YEkC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(stopwords.words('english'))\n",
        "punct = set(punctuation)\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "gPyqIooIYGYW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(txt):\n",
        "    #преобразуем входные данные в строку\n",
        "    txt = str(txt)\n",
        "    #удяляем знаки пунктуации\n",
        "    txt = \"\".join(c for c in txt if c not in punct)\n",
        "    #приводим все к нижнему регистру\n",
        "    txt = txt.lower()\n",
        "    #лематизация и удаление стоп-слов\n",
        "    txt = [lemmatizer.lemmatize(w) for w in txt.split() if w not in sw]\n",
        "    return \" \".join(txt)"
      ],
      "metadata": {
        "id": "BzlyYZy5Yh92"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
        "df_valid['tweet'] = df_valid['tweet'].progress_apply(preprocess_text)\n",
        "test_df['tweet'] = test_df['tweet'].progress_apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee7tWmWxYnas",
        "outputId": "78d53e6f-64f8-4da6-8e51-98364b3da1b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23971/23971 [00:04<00:00, 5701.74it/s]\n",
            "100%|██████████| 7991/7991 [00:01<00:00, 6992.03it/s]\n",
            "100%|██████████| 17197/17197 [00:02<00:00, 8586.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = \" \".join(df_train[\"tweet\"])\n",
        "train_corpus = train_corpus.lower()\n",
        "tokens = word_tokenize(train_corpus)\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYtXoBwGY5n7",
        "outputId": "755441af-c612-4495-8c28-4710bd05ef5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['weekend',\n",
              " 'world',\n",
              " 'really',\n",
              " 'going',\n",
              " 'bonkers',\n",
              " 'really',\n",
              " 'bad',\n",
              " 'atm',\n",
              " 'shooting',\n",
              " 'deathstroke']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_filtered = [word for word in tokens if word.isalnum()]"
      ],
      "metadata": {
        "id": "UKOm2YkcZEHE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist = FreqDist(tokens_filtered)"
      ],
      "metadata": {
        "id": "BW0zaH9Nbk3G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 20000\n",
        "max_len = 10\n",
        "num_classes = 1\n",
        "\n",
        "# обучение\n",
        "epochs = 10\n",
        "batch_size = 512\n",
        "print_batch_n = 100"
      ],
      "metadata": {
        "id": "4FcocSj-bmt5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # -1 - padding\n",
        "len(tokens_filtered_top), tokens_filtered_top[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_Fzw4iGbsFW",
        "outputId": "691fe4ec-5e93-4901-f40c-e8327fa811ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19999,\n",
              " ['user', 'love', 'day', 'u', 'happy', 'amp', 'time', 'life', 'im', 'today'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top,1)).items()}"
      ],
      "metadata": {
        "id": "P71Qz3aMbw5A"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, maxlen):\n",
        "    result = []\n",
        "    tokens = word_tokenize(text.lower())  #токенизация\n",
        "    tokens_filtered = [word for word in tokens if word.isalnum()] #фильтруем (только буквы и цифры)\n",
        "    for word in tokens_filtered:\n",
        "        if word in vocabulary:\n",
        "            result.append(vocabulary[word]) #если слово в топе токенов, то добавляем его индекс в результат\n",
        "\n",
        "    padding = [0] * (maxlen-len(result)) #нули дополняющие до maxlen\n",
        "    return result[-maxlen:] + padding"
      ],
      "metadata": {
        "id": "xUYZOTPnby7L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]], dtype=np.int32)\n",
        "x_valid = np.asarray([text_to_sequence(text, max_len) for text in df_valid[\"tweet\"]], dtype=np.int32)\n",
        "x_test = np.asarray([text_to_sequence(text, max_len) for text in test_df[\"tweet\"]], dtype=np.int32)\n",
        "\n",
        "x_train.shape, x_valid.shape, x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlCC0dmMb27A",
        "outputId": "1729d8d9-7d33-47d1-8dfd-3a2c9449f199"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23971, 10), (7991, 10), (17197, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataWrapper(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).long() #преобразуем в целочисленный тензор\n",
        "        self.target = torch.from_numpy(target).long() #преобразуем в целочисленный тензор\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]  #индексация данных\n",
        "        y = self.target[index]  #индексация целевой переменной\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "            \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "6ajvxwVDcBC1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataWrapper(x_train, df_train['label'].values)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = DataWrapper(x_valid, df_valid['label'].values)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "am7j3hoMcL0f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Построение и обучение нейронной сети"
      ],
      "metadata": {
        "id": "n-phM-pN1gBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUFixedLen(nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, use_last=True):\n",
        "        super().__init__()\n",
        "        self.use_last = use_last\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        gru_out, ht = self.gru(x)\n",
        "       \n",
        "        if self.use_last:\n",
        "            last_tensor = gru_out[:,-1,:]\n",
        "        else:\n",
        "            # use mean\n",
        "            last_tensor = torch.mean(gru_out[:,:], dim=1)\n",
        "    \n",
        "        out = self.linear(last_tensor)\n",
        "        return torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "63X9copD1noq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3mdlzCKo1w14",
        "outputId": "c64bfd32-1e68-4765-fe60-d797e20da8e8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUFixedLen(max_words).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "twGOK62i10Ny"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "th = 0.5\n",
        "\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs): \n",
        "    model.train() \n",
        "    running_items, running_right = 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # подсчет ошибки на обучении\n",
        "        loss = loss.item()\n",
        "        running_items += len(labels)\n",
        "        # подсчет метрики на обучении\n",
        "        pred_labels = torch.squeeze((outputs > th).int())\n",
        "        running_right += (labels == pred_labels).sum()\n",
        "        \n",
        "    # выводим статистику о процессе обучения\n",
        "    model.eval()\n",
        "    \n",
        "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
        "          f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "          f'Loss: {loss:.3f}. ' \\\n",
        "          f'Acc: {running_right / running_items:.3f}', end='. ')\n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    train_loss_history.append(loss)\n",
        "\n",
        "    # выводим статистику на тестовых данных\n",
        "    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
        "    for j, data in enumerate(valid_loader):\n",
        "        test_labels = data[1].to(device)\n",
        "        test_outputs = model(data[0].to(device))\n",
        "        \n",
        "        # подсчет ошибки на тесте\n",
        "        test_loss = criterion(test_outputs, test_labels.float().view(-1, 1))\n",
        "        # подсчет метрики на тесте\n",
        "        test_running_total += len(data[1])\n",
        "        pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
        "        test_running_right += (test_labels == pred_test_labels).sum()\n",
        "    \n",
        "    test_loss_history.append(test_loss.item())\n",
        "    print(f'Test loss: {test_loss:.3f}. Test acc: {test_running_right / test_running_total:.3f}')\n",
        "            \n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCpAfozQ16ke",
        "outputId": "0463580b-1727-4dac-bb9d-e8cca33c1607"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]. Step [47/47]. Loss: 0.244. Acc: 0.910. Test loss: 0.172. Test acc: 0.938\n",
            "Epoch [2/10]. Step [47/47]. Loss: 0.178. Acc: 0.941. Test loss: 0.036. Test acc: 0.944\n",
            "Epoch [3/10]. Step [47/47]. Loss: 0.181. Acc: 0.950. Test loss: 0.542. Test acc: 0.952\n",
            "Epoch [4/10]. Step [47/47]. Loss: 0.109. Acc: 0.958. Test loss: 0.014. Test acc: 0.955\n",
            "Epoch [5/10]. Step [47/47]. Loss: 0.084. Acc: 0.967. Test loss: 0.022. Test acc: 0.955\n",
            "Epoch [6/10]. Step [47/47]. Loss: 0.132. Acc: 0.972. Test loss: 0.011. Test acc: 0.957\n",
            "Epoch [7/10]. Step [47/47]. Loss: 0.066. Acc: 0.977. Test loss: 0.005. Test acc: 0.955\n",
            "Epoch [8/10]. Step [47/47]. Loss: 0.025. Acc: 0.980. Test loss: 0.002. Test acc: 0.955\n",
            "Epoch [9/10]. Step [47/47]. Loss: 0.018. Acc: 0.985. Test loss: 0.002. Test acc: 0.959\n",
            "Epoch [10/10]. Step [47/47]. Loss: 0.053. Acc: 0.986. Test loss: 0.002. Test acc: 0.958\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность модели на основе GRU сети с первого раза получилась довольно хорошей. Видно, что переобучение отсутствует."
      ],
      "metadata": {
        "id": "why8KJks2tRd"
      }
    }
  ]
}